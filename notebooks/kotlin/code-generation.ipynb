{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Adding dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T08:43:15.490839991Z",
     "start_time": "2026-01-03T08:43:15.400358664Z"
    },
    "collapsed": true,
    "executionRelatedData": {
     "compiledClasses": [
      "Line_13_jupyter",
      "Line_28_jupyter",
      "Line_42_jupyter",
      "Line_59_jupyter",
      "Line_27_jupyter",
      "Line_11_jupyter",
      "Line_18_jupyter",
      "Line_24_jupyter",
      "Line_10_jupyter",
      "Line_8_jupyter",
      "Line_19_jupyter",
      "Line_41_jupyter",
      "Line_37_jupyter",
      "Line_23_jupyter",
      "Line_3_jupyter",
      "Line_20_jupyter",
      "Line_36_jupyter",
      "Line_5_jupyter",
      "Line_34_jupyter",
      "Line_52_jupyter",
      "Line_17_jupyter",
      "Line_31_jupyter",
      "Line_47_jupyter",
      "Line_16_jupyter",
      "Line_29_jupyter"
     ]
    }
   },
   "outputs": [],
   "source": [
    "@file:DependsOn(\"dev.langchain4j:langchain4j:1.6.0\")\n",
    "@file:DependsOn(\"dev.langchain4j:langchain4j-open-ai:1.6.0\")\n",
    "@file:DependsOn(\"dev.langchain4j:langchain4j-http-client-jdk:1.6.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Setup networking layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T08:43:15.536789830Z",
     "start_time": "2026-01-03T08:43:15.492643764Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_13_jupyter",
      "Line_28_jupyter",
      "Line_42_jupyter",
      "Line_57_jupyter",
      "Line_11_jupyter",
      "Line_58_jupyter",
      "Line_9_jupyter",
      "Line_12_jupyter",
      "Line_60_jupyter",
      "Line_43_jupyter",
      "Line_18_jupyter",
      "Line_25_jupyter",
      "Line_24_jupyter",
      "Line_7_jupyter",
      "Line_53_jupyter",
      "Line_19_jupyter",
      "Line_8_jupyter",
      "Line_22_jupyter",
      "Line_6_jupyter",
      "Line_38_jupyter",
      "Line_37_jupyter",
      "Line_20_jupyter",
      "Line_21_jupyter",
      "Line_5_jupyter",
      "Line_35_jupyter",
      "Line_4_jupyter",
      "Line_32_jupyter",
      "Line_48_jupyter",
      "Line_16_jupyter",
      "Line_30_jupyter",
      "Line_14_jupyter",
      "Line_29_jupyter"
     ]
    }
   },
   "outputs": [],
   "source": [
    "import java.time.Duration\n",
    "import java.net.http.HttpClient\n",
    "import dev.langchain4j.model.openai.OpenAiChatModel\n",
    "import dev.langchain4j.http.client.jdk.JdkHttpClient\n",
    "\n",
    "val durationLimit = Duration.ofMinutes(20)\n",
    "val httpClientBuilder = HttpClient.newBuilder()\n",
    "    .version(HttpClient.Version.HTTP_1_1)\n",
    "\n",
    "val jdkHttpClientBuilder = JdkHttpClient.builder()\n",
    "    .httpClientBuilder(httpClientBuilder)\n",
    "\n",
    "// Connect to LM Studio Server\n",
    "val modelBuilder = OpenAiChatModel.builder()\n",
    "    .baseUrl(\"http://127.0.0.1:1234/v1\")\n",
    "    .httpClientBuilder(jdkHttpClientBuilder)\n",
    "    .timeout(durationLimit)\n",
    "    .temperature(0.0)\n",
    "    .returnThinking(false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Setup AI layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T08:43:15.589228623Z",
     "start_time": "2026-01-03T08:43:15.538057781Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_13_jupyter",
      "Line_20_jupyter",
      "Line_21_jupyter",
      "Line_44_jupyter",
      "Line_36_jupyter",
      "Line_5_jupyter",
      "Line_11_jupyter",
      "Line_49_jupyter",
      "Line_9_jupyter",
      "Line_26_jupyter",
      "Line_18_jupyter",
      "Line_24_jupyter",
      "Line_31_jupyter",
      "Line_61_jupyter",
      "Line_10_jupyter",
      "Line_39_jupyter",
      "Line_8_jupyter",
      "Line_19_jupyter",
      "Line_30_jupyter",
      "Line_33_jupyter",
      "Line_15_jupyter",
      "Line_54_jupyter",
      "Line_23_jupyter",
      "Line_25_jupyter"
     ]
    }
   },
   "outputs": [],
   "source": [
    "import dev.langchain4j.service.Result\n",
    "\n",
    "val modelList = listOf(\n",
    "    \"microsoft/phi-4\",\n",
    "    \"openai/gpt-oss-20b\",\n",
    "    \"mistralai/devstral-small-2-2512\",\n",
    "    \"google/gemma-3-27b\",\n",
    "    \"qwen/qwen3-coder-30b\",\n",
    ")\n",
    "\n",
    "interface CodeGenAiService {\n",
    "    fun generateCode(prompt: String): Result<String>\n",
    "}\n",
    "\n",
    "data class Task(\n",
    "    val systemPromptPath: String = \"\",\n",
    "    val promptPath: String = \"\",\n",
    "    val outputDirectory: String = \"\",\n",
    "    val extension: String = \"\",\n",
    ")\n",
    "\n",
    "val basePromptPath = \"../../resources/prompts\"\n",
    "val baseBuildPath = \"../../build\"\n",
    "\n",
    "val taskList = listOf<Task>(\n",
    "    Task(\"$basePromptPath/system-prompt-kotlin.md\", \"$basePromptPath/test1-preview.md\", \"$baseBuildPath/test1-preview\", \"kt\"),\n",
    "    Task(\"$basePromptPath/system-prompt-kotlin.md\", \"$basePromptPath/test2-unit-test.md\", \"$baseBuildPath/test2-unit-test\", \"kt\"),\n",
    "    Task(\"$basePromptPath/system-prompt-kotlin.md\", \"$basePromptPath/test3-instrumentation-test.md\", \"$baseBuildPath/test3-instrumentation-test\", \"kt\"),\n",
    "    Task(\"$basePromptPath/system-prompt-diff.md\", \"$basePromptPath/test4-deprecated-material.md\", \"$baseBuildPath/test4-deprecated-material\", \"diff\"),\n",
    "    Task(\"$basePromptPath/system-prompt-diff.md\", \"$basePromptPath/test5-deprecated-plugin.md\", \"$baseBuildPath/test5-deprecated-plugin\", \"diff\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Setup resource monitor and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T08:43:15.764173825Z",
     "start_time": "2026-01-03T08:43:15.589832973Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_27_jupyter",
      "Line_11_jupyter",
      "Line_9_jupyter",
      "Line_43_jupyter",
      "Line_26_jupyter",
      "Line_25_jupyter",
      "Line_55_jupyter",
      "Line_10_jupyter",
      "Line_22_jupyter",
      "Line_38_jupyter",
      "Line_6_jupyter",
      "Line_37_jupyter",
      "Line_23_jupyter",
      "Line_40_jupyter",
      "Line_20_jupyter",
      "Line_50_jupyter",
      "Line_21_jupyter",
      "Line_5_jupyter",
      "Line_34_jupyter",
      "Line_32_jupyter",
      "Line_62_jupyter",
      "Line_16_jupyter",
      "Line_30_jupyter",
      "Line_15_jupyter",
      "Line_45_jupyter",
      "Line_14_jupyter"
     ]
    }
   },
   "outputs": [],
   "source": [
    "import java.io.File\n",
    "import kotlin.concurrent.thread\n",
    "import kotlin.io.path.Path\n",
    "import kotlin.io.path.createDirectories\n",
    "import kotlin.io.path.pathString\n",
    "import kotlin.io.path.writeText\n",
    "\n",
    "data class PeakStats<T>(\n",
    "    val result: T,\n",
    "    val durationSeconds: UInt,\n",
    "    val startRamGb: Double,\n",
    "    val peakRamGb: Double,\n",
    "    val startVramGb: Double,\n",
    "    val peakVramGb: Double\n",
    ")\n",
    "\n",
    "class ResourceMonitor {\n",
    "\n",
    "    private val vramFile: File? by lazy {\n",
    "        File(\"/sys/class/drm\").listFiles()\n",
    "            ?.filter { it.name.startsWith(\"card\") && !it.name.contains(\"-\") }\n",
    "            ?.maxByOrNull { card ->\n",
    "                File(card, \"device/mem_info_vram_total\").let {\n",
    "                    if (it.exists()) it.readText().trim().toLongOrNull() ?: 0L else 0L\n",
    "                }\n",
    "            }?.let { File(it, \"device/mem_info_vram_used\") }\n",
    "    }\n",
    "\n",
    "    fun <T> measurePeakDelta(block: () -> T): PeakStats<T> {\n",
    "        val startRam = getUsedRamGb()\n",
    "        val startVram = getUsedVramGb()\n",
    "\n",
    "        var peakRam = startRam\n",
    "        var peakVram = startVram\n",
    "        var running = true\n",
    "\n",
    "        val monitorThread = thread {\n",
    "            while (running) {\n",
    "                peakRam = max(peakRam, getUsedRamGb())\n",
    "                peakVram = max(peakVram, getUsedVramGb())\n",
    "                Thread.sleep(100)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        val startTime = System.currentTimeMillis()\n",
    "        val result = try {\n",
    "            block()\n",
    "        } finally {\n",
    "            running = false\n",
    "            monitorThread.join()\n",
    "        }\n",
    "        val durationSeconds = ((System.currentTimeMillis() - startTime) / 1000).toUInt()\n",
    "\n",
    "        return PeakStats(\n",
    "            result = result,\n",
    "            durationSeconds = durationSeconds,\n",
    "            startRamGb = startRam,\n",
    "            peakRamGb = peakRam,\n",
    "            startVramGb = startVram,\n",
    "            peakVramGb = peakVram\n",
    "        )\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * Returns actual RAM used by applications (excluding buffers/cache).\n",
    "     * Uses MemTotal - MemAvailable from /proc/meminfo for accurate measurement.\n",
    "     */\n",
    "    private fun getUsedRamGb(): Double {\n",
    "        val memInfo = File(\"/proc/meminfo\").readLines()\n",
    "            .mapNotNull { line ->\n",
    "                val parts = line.split(\":\", limit = 2)\n",
    "                if (parts.size == 2) {\n",
    "                    val key = parts[0].trim()\n",
    "                    val value = parts[1].trim().split(\" \")[0].toLongOrNull()\n",
    "                    if (value != null) key to value else null\n",
    "                } else null\n",
    "            }\n",
    "            .toMap()\n",
    "\n",
    "        val total = memInfo[\"MemTotal\"] ?: return 0.0\n",
    "        val available = memInfo[\"MemAvailable\"] ?: return 0.0\n",
    "\n",
    "        // Convert from KB to GB\n",
    "        return (total - available) / (1024.0 * 1024.0)\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * Returns VRAM used by AMD GPU in GB.\n",
    "     * Reads from /sys/class/drm/cardX/device/mem_info_vram_used\n",
    "     */\n",
    "    private fun getUsedVramGb(): Double {\n",
    "        val bytes = vramFile?.readText()?.trim()?.toLongOrNull() ?: 0L\n",
    "        return bytes / (1024.0 * 1024.0 * 1024.0)\n",
    "    }\n",
    "}\n",
    "\n",
    "fun String.saveToFile(folderName: String, outputName: String) {\n",
    "    val folderPath = Path(folderName)\n",
    "    folderPath.createDirectories()\n",
    "\n",
    "    val filePath = Path(\"$folderPath/$outputName\")\n",
    "    filePath.writeText(this)\n",
    "\n",
    "    println(\"Saved to: ${filePath.pathString}\")\n",
    "}\n",
    "\n",
    "fun String.sanitizeForFilename(): String = replace(\"/\", \"_\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Execute and store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T09:35:59.722412163Z",
     "start_time": "2026-01-03T08:43:15.766029374Z"
    },
    "executionRelatedData": {
     "compiledClasses": [
      "Line_32_jupyter",
      "Line_8_jupyter",
      "Line_33_jupyter",
      "Line_15_jupyter",
      "Line_22_jupyter",
      "Line_6_jupyter",
      "Line_45_jupyter",
      "Line_12_jupyter",
      "Line_26_jupyter",
      "Line_27_jupyter"
     ]
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ../../build/test1-preview/result1-microsoft_phi-4.kt\n",
      "Saved to: ../../build/test1-preview/result2-openai_gpt-oss-20b.kt\n",
      "Saved to: ../../build/test1-preview/result3-mistralai_devstral-small-2-2512.kt\n",
      "Saved to: ../../build/test1-preview/result4-google_gemma-3-27b.kt\n",
      "Saved to: ../../build/test1-preview/result5-qwen_qwen3-coder-30b.kt\n",
      "Saved to: ../../build/test1-preview/execution-results.csv\n",
      "Saved to: ../../build/test2-unit-test/result1-microsoft_phi-4.kt\n",
      "Saved to: ../../build/test2-unit-test/result2-openai_gpt-oss-20b.kt\n",
      "Saved to: ../../build/test2-unit-test/result3-mistralai_devstral-small-2-2512.kt\n",
      "Saved to: ../../build/test2-unit-test/result4-google_gemma-3-27b.kt\n",
      "Saved to: ../../build/test2-unit-test/result5-qwen_qwen3-coder-30b.kt\n",
      "Saved to: ../../build/test2-unit-test/execution-results.csv\n",
      "Saved to: ../../build/test3-instrumentation-test/result1-microsoft_phi-4.kt\n",
      "Saved to: ../../build/test3-instrumentation-test/result2-openai_gpt-oss-20b.kt\n",
      "Saved to: ../../build/test3-instrumentation-test/result3-mistralai_devstral-small-2-2512.kt\n",
      "Saved to: ../../build/test3-instrumentation-test/result4-google_gemma-3-27b.kt\n",
      "Saved to: ../../build/test3-instrumentation-test/result5-qwen_qwen3-coder-30b.kt\n",
      "Saved to: ../../build/test3-instrumentation-test/execution-results.csv\n",
      "Saved to: ../../build/test4-deprecated-material/result1-microsoft_phi-4.diff\n",
      "Saved to: ../../build/test4-deprecated-material/result2-openai_gpt-oss-20b.diff\n",
      "Saved to: ../../build/test4-deprecated-material/result3-mistralai_devstral-small-2-2512.diff\n",
      "Saved to: ../../build/test4-deprecated-material/result4-google_gemma-3-27b.diff\n",
      "Saved to: ../../build/test4-deprecated-material/result5-qwen_qwen3-coder-30b.diff\n",
      "Saved to: ../../build/test4-deprecated-material/execution-results.csv\n",
      "Saved to: ../../build/test5-deprecated-plugin/result1-microsoft_phi-4.diff\n",
      "Saved to: ../../build/test5-deprecated-plugin/result2-openai_gpt-oss-20b.diff\n",
      "Saved to: ../../build/test5-deprecated-plugin/result3-mistralai_devstral-small-2-2512.diff\n",
      "Saved to: ../../build/test5-deprecated-plugin/result4-google_gemma-3-27b.diff\n",
      "Saved to: ../../build/test5-deprecated-plugin/result5-qwen_qwen3-coder-30b.diff\n",
      "Saved to: ../../build/test5-deprecated-plugin/execution-results.csv\n"
     ]
    }
   ],
   "source": [
    "import dev.langchain4j.service.AiServices\n",
    "\n",
    "data class ModelExecutionResult(\n",
    "    val modelName: String,\n",
    "    val durationSeconds: UInt,\n",
    "    val inputTokenCount: UInt,\n",
    "    val outputTokenCount: UInt,\n",
    "    val totalTokenCount: UInt,\n",
    "    val startRamGb: Double,\n",
    "    val peakRamGb: Double,\n",
    "    val startVramGb: Double,\n",
    "    val peakVramGb: Double,\n",
    "    val resultPath: String,\n",
    ") {\n",
    "    fun toCsvRow(): String =\n",
    "        \"$modelName,$durationSeconds,$inputTokenCount,$outputTokenCount,$totalTokenCount,\" +\n",
    "                \"${\"%.2f\".format(startRamGb)},${\"%.2f\".format(peakRamGb)},\" +\n",
    "                \"${\"%.2f\".format(startVramGb)},${\"%.2f\".format(peakVramGb)},$resultPath\"\n",
    "\n",
    "    companion object {\n",
    "        const val CSV_HEADER = \"modelName,durationSeconds,inputTokenCount,outputTokenCount,totalTokenCount,startRamGb,peakRamGb,startVramGb,peakVramGb,resultPath\"\n",
    "    }\n",
    "}\n",
    "\n",
    "val monitor = ResourceMonitor()\n",
    "\n",
    "fun createService(\n",
    "    modelName: String,\n",
    "    systemPromptPath: String,\n",
    "): CodeGenAiService {\n",
    "    val model = modelBuilder.modelName(modelName).build()\n",
    "    val systemPrompt = File(systemPromptPath).readText().trimIndent()\n",
    "    return AiServices.builder(CodeGenAiService::class.java)\n",
    "        .systemMessageProvider { systemPrompt }\n",
    "        .chatModel(model)\n",
    "        .build()\n",
    "}\n",
    "\n",
    "taskList.forEach { task ->\n",
    "    modelList.mapIndexed { index, modelName ->\n",
    "        val service = createService(modelName, task.systemPromptPath)\n",
    "        val userPrompt = File(task.promptPath).readText().trimIndent()\n",
    "        val stats: PeakStats<Result<String>> = monitor.measurePeakDelta { service.generateCode(userPrompt) }\n",
    "\n",
    "        val path = \"result${index + 1}-${modelName.sanitizeForFilename()}.${task.extension}\"\n",
    "        stats.result.content().saveToFile(task.outputDirectory, path)\n",
    "\n",
    "        ModelExecutionResult(\n",
    "            modelName = modelName,\n",
    "            inputTokenCount = stats.result.tokenUsage().inputTokenCount().toUInt(),\n",
    "            outputTokenCount = stats.result.tokenUsage().outputTokenCount().toUInt(),\n",
    "            totalTokenCount = stats.result.tokenUsage().totalTokenCount().toUInt(),\n",
    "            durationSeconds = stats.durationSeconds,\n",
    "            startRamGb = stats.startRamGb,\n",
    "            peakRamGb = stats.peakRamGb,\n",
    "            startVramGb = stats.startVramGb,\n",
    "            peakVramGb = stats.peakVramGb,\n",
    "            resultPath = path\n",
    "        ).also {\n",
    "            // Allow VRAM to goes back to normal\n",
    "            Thread.sleep(80_000)\n",
    "        }\n",
    "    }.joinToString(\n",
    "        separator = \"\\n\",\n",
    "        prefix = \"${ModelExecutionResult.CSV_HEADER}\\n\",\n",
    "        transform = ModelExecutionResult::toCsvRow\n",
    "    ).also { it.saveToFile(task.outputDirectory, \"execution-results.csv\") }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "2.2.20-Beta2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
