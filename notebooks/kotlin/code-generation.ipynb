{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "1. Adding dependencies:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "executionRelatedData": {
     "compiledClasses": [
      "Line_3_jupyter",
      "Line_20_jupyter",
      "Line_13_jupyter",
      "Line_28_jupyter",
      "Line_42_jupyter",
      "Line_59_jupyter",
      "Line_36_jupyter",
      "Line_5_jupyter",
      "Line_27_jupyter",
      "Line_52_jupyter",
      "Line_11_jupyter",
      "Line_18_jupyter",
      "Line_17_jupyter",
      "Line_31_jupyter",
      "Line_24_jupyter",
      "Line_47_jupyter",
      "Line_10_jupyter",
      "Line_16_jupyter",
      "Line_8_jupyter",
      "Line_19_jupyter",
      "Line_41_jupyter",
      "Line_37_jupyter",
      "Line_23_jupyter",
      "Line_29_jupyter",
      "Line_34_jupyter"
     ]
    },
    "ExecuteTime": {
     "end_time": "2026-01-03T06:23:57.422554854Z",
     "start_time": "2026-01-03T06:23:57.215883094Z"
    }
   },
   "source": [
    "@file:DependsOn(\"dev.langchain4j:langchain4j:1.6.0\")\n",
    "@file:DependsOn(\"dev.langchain4j:langchain4j-open-ai:1.6.0\")\n",
    "@file:DependsOn(\"dev.langchain4j:langchain4j-http-client-jdk:1.6.0\")"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "2. Setup networking layer"
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionRelatedData": {
     "compiledClasses": [
      "Line_13_jupyter",
      "Line_28_jupyter",
      "Line_42_jupyter",
      "Line_57_jupyter",
      "Line_11_jupyter",
      "Line_58_jupyter",
      "Line_9_jupyter",
      "Line_12_jupyter",
      "Line_60_jupyter",
      "Line_43_jupyter",
      "Line_18_jupyter",
      "Line_25_jupyter",
      "Line_24_jupyter",
      "Line_7_jupyter",
      "Line_53_jupyter",
      "Line_19_jupyter",
      "Line_8_jupyter",
      "Line_22_jupyter",
      "Line_6_jupyter",
      "Line_38_jupyter",
      "Line_37_jupyter",
      "Line_20_jupyter",
      "Line_21_jupyter",
      "Line_5_jupyter",
      "Line_4_jupyter",
      "Line_32_jupyter",
      "Line_48_jupyter",
      "Line_16_jupyter",
      "Line_30_jupyter",
      "Line_14_jupyter",
      "Line_29_jupyter",
      "Line_35_jupyter"
     ]
    },
    "ExecuteTime": {
     "end_time": "2026-01-03T06:23:57.540060296Z",
     "start_time": "2026-01-03T06:23:57.423465148Z"
    }
   },
   "source": [
    "import java.time.Duration\n",
    "import java.net.http.HttpClient\n",
    "import dev.langchain4j.model.openai.OpenAiChatModel\n",
    "import dev.langchain4j.http.client.jdk.JdkHttpClient\n",
    "\n",
    "val durationLimit = Duration.ofMinutes(20)\n",
    "val httpClientBuilder = HttpClient.newBuilder()\n",
    "    .version(HttpClient.Version.HTTP_1_1)\n",
    "\n",
    "val jdkHttpClientBuilder = JdkHttpClient.builder()\n",
    "    .httpClientBuilder(httpClientBuilder)\n",
    "\n",
    "// Connect to LM Studio Server\n",
    "val modelBuilder = OpenAiChatModel.builder()\n",
    "    .baseUrl(\"http://127.0.0.1:1234/v1\")\n",
    "    .httpClientBuilder(jdkHttpClientBuilder)\n",
    "    .timeout(durationLimit)\n",
    "    .temperature(0.0)\n",
    "    .returnThinking(false)\n"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. Setup AI layer"
  },
  {
   "metadata": {
    "executionRelatedData": {
     "compiledClasses": [
      "Line_13_jupyter",
      "Line_21_jupyter",
      "Line_44_jupyter",
      "Line_36_jupyter",
      "Line_11_jupyter",
      "Line_49_jupyter",
      "Line_9_jupyter",
      "Line_26_jupyter",
      "Line_18_jupyter",
      "Line_24_jupyter",
      "Line_31_jupyter",
      "Line_61_jupyter",
      "Line_10_jupyter",
      "Line_39_jupyter",
      "Line_8_jupyter",
      "Line_19_jupyter",
      "Line_30_jupyter",
      "Line_33_jupyter",
      "Line_54_jupyter",
      "Line_23_jupyter"
     ]
    },
    "ExecuteTime": {
     "end_time": "2026-01-03T06:23:57.692942819Z",
     "start_time": "2026-01-03T06:23:57.541215598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dev.langchain4j.service.Result\n",
    "\n",
    "val modelList = listOf(\n",
    "    \"microsoft/phi-4\",\n",
    "    \"openai/gpt-oss-20b\",\n",
    "    \"mistralai/devstral-small-2-2512\",\n",
    "    \"google/gemma-3-27b\",\n",
    "    \"qwen/qwen3-coder-30b\",\n",
    ")\n",
    "\n",
    "interface CodeGenAiService {\n",
    "    fun generateCode(prompt: String): Result<String>\n",
    "}\n",
    "\n",
    "data class Task(\n",
    "    val systemPromptPath: String = \"\",\n",
    "    val promptPath: String = \"\",\n",
    "    val outputDirectory: String = \"\",\n",
    "    val extension: String = \"\",\n",
    ")\n",
    "\n",
    "val basePromptPath = \"../../resources/prompts\"\n",
    "val baseBuildPath = \"../../build\"\n",
    "\n",
    "val taskList = listOf<Task>(\n",
    "    Task(\"$basePromptPath/system-prompt-kotlin.md\", \"$basePromptPath/test1-preview.md\", \"$baseBuildPath/test1-preview\", \"kt\"),\n",
    "    Task(\"$basePromptPath/system-prompt-kotlin.md\", \"$basePromptPath/test2-unit-test.md\", \"$baseBuildPath/test2-unit-test\", \"kt\"),\n",
    "    Task(\"$basePromptPath/system-prompt-kotlin.md\", \"$basePromptPath/test3-instrumentation-test.md\", \"$baseBuildPath/test3-instrumentation-test\", \"kt\"),\n",
    "    Task(\"$basePromptPath/system-prompt-diff.md\", \"$basePromptPath/test4-deprecated-material.md\", \"$baseBuildPath/test4-deprecated-material\", \"diff\"),\n",
    "    Task(\"$basePromptPath/system-prompt-diff.md\", \"$basePromptPath/test5-deprecated-plugin.md\", \"$baseBuildPath/test5-deprecated-plugin\", \"diff\"),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "4. Setup resource monitor and utilities"
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionRelatedData": {
     "compiledClasses": [
      "Line_20_jupyter",
      "Line_50_jupyter",
      "Line_5_jupyter",
      "Line_34_jupyter",
      "Line_27_jupyter",
      "Line_9_jupyter",
      "Line_43_jupyter",
      "Line_26_jupyter",
      "Line_25_jupyter",
      "Line_32_jupyter",
      "Line_55_jupyter",
      "Line_62_jupyter",
      "Line_10_jupyter",
      "Line_16_jupyter",
      "Line_30_jupyter",
      "Line_15_jupyter",
      "Line_22_jupyter",
      "Line_38_jupyter",
      "Line_45_jupyter",
      "Line_14_jupyter",
      "Line_23_jupyter",
      "Line_40_jupyter",
      "Line_37_jupyter"
     ]
    },
    "ExecuteTime": {
     "end_time": "2026-01-03T06:23:58.191627593Z",
     "start_time": "2026-01-03T06:23:57.694850930Z"
    }
   },
   "source": [
    "import java.io.File\n",
    "import kotlin.concurrent.thread\n",
    "import kotlin.io.path.Path\n",
    "import kotlin.io.path.absolutePathString\n",
    "import kotlin.io.path.createDirectories\n",
    "import kotlin.io.path.writeText\n",
    "\n",
    "data class PeakStats<T>(\n",
    "    val result: T,\n",
    "    val durationSeconds: UInt,\n",
    "    val startRamGb: Double,\n",
    "    val peakRamGb: Double,\n",
    "    val startVramGb: Double,\n",
    "    val peakVramGb: Double\n",
    ")\n",
    "\n",
    "class ResourceMonitor {\n",
    "\n",
    "    private val vramFile: File? by lazy {\n",
    "        File(\"/sys/class/drm\").listFiles()\n",
    "            ?.filter { it.name.startsWith(\"card\") && !it.name.contains(\"-\") }\n",
    "            ?.maxByOrNull { card ->\n",
    "                File(card, \"device/mem_info_vram_total\").let {\n",
    "                    if (it.exists()) it.readText().trim().toLongOrNull() ?: 0L else 0L\n",
    "                }\n",
    "            }?.let { File(it, \"device/mem_info_vram_used\") }\n",
    "    }\n",
    "\n",
    "    fun <T> measurePeakDelta(block: () -> T): PeakStats<T> {\n",
    "        val startRam = getUsedRamGb()\n",
    "        val startVram = getUsedVramGb()\n",
    "\n",
    "        var peakRam = startRam\n",
    "        var peakVram = startVram\n",
    "        var running = true\n",
    "\n",
    "        val monitorThread = thread {\n",
    "            while (running) {\n",
    "                peakRam = max(peakRam, getUsedRamGb())\n",
    "                peakVram = max(peakVram, getUsedVramGb())\n",
    "                Thread.sleep(100)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        val startTime = System.currentTimeMillis()\n",
    "        val result = try {\n",
    "            block()\n",
    "        } finally {\n",
    "            running = false\n",
    "            monitorThread.join()\n",
    "        }\n",
    "        val durationSeconds = ((System.currentTimeMillis() - startTime) / 1000).toUInt()\n",
    "\n",
    "        return PeakStats(\n",
    "            result = result,\n",
    "            durationSeconds = durationSeconds,\n",
    "            startRamGb = startRam,\n",
    "            peakRamGb = peakRam,\n",
    "            startVramGb = startVram,\n",
    "            peakVramGb = peakVram\n",
    "        )\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * Returns actual RAM used by applications (excluding buffers/cache).\n",
    "     * Uses MemTotal - MemAvailable from /proc/meminfo for accurate measurement.\n",
    "     */\n",
    "    private fun getUsedRamGb(): Double {\n",
    "        val memInfo = File(\"/proc/meminfo\").readLines()\n",
    "            .mapNotNull { line ->\n",
    "                val parts = line.split(\":\", limit = 2)\n",
    "                if (parts.size == 2) {\n",
    "                    val key = parts[0].trim()\n",
    "                    val value = parts[1].trim().split(\" \")[0].toLongOrNull()\n",
    "                    if (value != null) key to value else null\n",
    "                } else null\n",
    "            }\n",
    "            .toMap()\n",
    "\n",
    "        val total = memInfo[\"MemTotal\"] ?: return 0.0\n",
    "        val available = memInfo[\"MemAvailable\"] ?: return 0.0\n",
    "\n",
    "        // Convert from KB to GB\n",
    "        return (total - available) / (1024.0 * 1024.0)\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * Returns VRAM used by AMD GPU in GB.\n",
    "     * Reads from /sys/class/drm/cardX/device/mem_info_vram_used\n",
    "     */\n",
    "    private fun getUsedVramGb(): Double {\n",
    "        val bytes = vramFile?.readText()?.trim()?.toLongOrNull() ?: 0L\n",
    "        return bytes / (1024.0 * 1024.0 * 1024.0)\n",
    "    }\n",
    "}\n",
    "\n",
    "fun String.saveToFile(folderName: String, outputName: String) {\n",
    "    val folderPath = Path(folderName)\n",
    "    folderPath.createDirectories()\n",
    "\n",
    "    val filePath = Path(\"$folderPath/$outputName\")\n",
    "    filePath.writeText(this)\n",
    "\n",
    "    println(\"Saved to: ${filePath.absolutePathString()}\")\n",
    "}\n",
    "\n",
    "fun String.sanitizeForFilename(): String = replace(\"/\", \"_\")\n"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "5. Execute and store the results"
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionRelatedData": {
     "compiledClasses": [
      "Line_32_jupyter",
      "Line_8_jupyter",
      "Line_33_jupyter",
      "Line_15_jupyter",
      "Line_22_jupyter",
      "Line_6_jupyter",
      "Line_45_jupyter",
      "Line_12_jupyter",
      "Line_26_jupyter"
     ]
    },
    "ExecuteTime": {
     "end_time": "2026-01-03T07:27:17.606442999Z",
     "start_time": "2026-01-03T06:23:58.198839468Z"
    }
   },
   "source": [
    "import dev.langchain4j.service.AiServices\n",
    "\n",
    "data class ModelExecutionResult(\n",
    "    val modelName: String,\n",
    "    val durationSeconds: UInt,\n",
    "    val inputTokenCount: UInt,\n",
    "    val outputTokenCount: UInt,\n",
    "    val totalTokenCount: UInt,\n",
    "    val startRamGb: Double,\n",
    "    val peakRamGb: Double,\n",
    "    val startVramGb: Double,\n",
    "    val peakVramGb: Double,\n",
    "    val resultPath: String,\n",
    ") {\n",
    "    fun toCsvRow(): String =\n",
    "        \"$modelName,$durationSeconds,$inputTokenCount,$outputTokenCount,$totalTokenCount,\" +\n",
    "                \"${\"%.2f\".format(startRamGb)},${\"%.2f\".format(peakRamGb)},\" +\n",
    "                \"${\"%.2f\".format(startVramGb)},${\"%.2f\".format(peakVramGb)},$resultPath\"\n",
    "\n",
    "    companion object {\n",
    "        const val CSV_HEADER = \"modelName,durationSeconds,inputTokenCount,outputTokenCount,totalTokenCount,startRamGb,peakRamGb,startVramGb,peakVramGb,resultPath\"\n",
    "    }\n",
    "}\n",
    "\n",
    "val monitor = ResourceMonitor()\n",
    "\n",
    "fun createService(\n",
    "    modelName: String,\n",
    "    systemPromptPath: String,\n",
    "): CodeGenAiService {\n",
    "    val model = modelBuilder.modelName(modelName).build()\n",
    "    val systemPrompt = File(systemPromptPath).readText().trimIndent()\n",
    "    return AiServices.builder(CodeGenAiService::class.java)\n",
    "        .systemMessageProvider { systemPrompt }\n",
    "        .chatModel(model)\n",
    "        .build()\n",
    "}\n",
    "\n",
    "taskList.forEach { task ->\n",
    "    modelList.mapIndexed { index, modelName ->\n",
    "        val service = createService(modelName, task.systemPromptPath)\n",
    "        val userPrompt = File(task.promptPath).readText().trimIndent()\n",
    "        val stats: PeakStats<Result<String>> = monitor.measurePeakDelta { service.generateCode(userPrompt) }\n",
    "\n",
    "        val path = \"result${index + 1}-${modelName.sanitizeForFilename()}.${task.extension}\"\n",
    "        stats.result.content().saveToFile(task.outputDirectory, path)\n",
    "\n",
    "        ModelExecutionResult(\n",
    "            modelName = modelName,\n",
    "            inputTokenCount = stats.result.tokenUsage().inputTokenCount().toUInt(),\n",
    "            outputTokenCount = stats.result.tokenUsage().outputTokenCount().toUInt(),\n",
    "            totalTokenCount = stats.result.tokenUsage().totalTokenCount().toUInt(),\n",
    "            durationSeconds = stats.durationSeconds,\n",
    "            startRamGb = stats.startRamGb,\n",
    "            peakRamGb = stats.peakRamGb,\n",
    "            startVramGb = stats.startVramGb,\n",
    "            peakVramGb = stats.peakVramGb,\n",
    "            resultPath = path\n",
    "        ).also {\n",
    "            // Allow VRAM to goes back to normal\n",
    "            Thread.sleep(80_000)\n",
    "        }\n",
    "    }.joinToString(\n",
    "        separator = \"\\n\",\n",
    "        prefix = \"${ModelExecutionResult.CSV_HEADER}\\n\",\n",
    "        transform = ModelExecutionResult::toCsvRow\n",
    "    ).also { it.saveToFile(task.outputDirectory, \"execution-results.csv\") }\n",
    "}"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test1-preview/result1-mellum-4b-sft-kotlin.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test1-preview/result2-microsoft_phi-4.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test1-preview/result3-openai_gpt-oss-20b.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test1-preview/result4-mistralai_devstral-small-2-2512.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test1-preview/result5-google_gemma-3-27b.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test1-preview/result6-qwen_qwen3-coder-30b.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test1-preview/result7-nvidia_nemotron-3-nano.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test1-preview/execution-results.csv\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test2-unit-test/result1-mellum-4b-sft-kotlin.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test2-unit-test/result2-microsoft_phi-4.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test2-unit-test/result3-openai_gpt-oss-20b.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test2-unit-test/result4-mistralai_devstral-small-2-2512.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test2-unit-test/result5-google_gemma-3-27b.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test2-unit-test/result6-qwen_qwen3-coder-30b.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test2-unit-test/result7-nvidia_nemotron-3-nano.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test2-unit-test/execution-results.csv\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test3-instrumentation-test/result1-mellum-4b-sft-kotlin.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test3-instrumentation-test/result2-microsoft_phi-4.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test3-instrumentation-test/result3-openai_gpt-oss-20b.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test3-instrumentation-test/result4-mistralai_devstral-small-2-2512.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test3-instrumentation-test/result5-google_gemma-3-27b.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test3-instrumentation-test/result6-qwen_qwen3-coder-30b.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test3-instrumentation-test/result7-nvidia_nemotron-3-nano.kt\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test3-instrumentation-test/execution-results.csv\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test4-deprecated-material/result1-mellum-4b-sft-kotlin.diff\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test4-deprecated-material/result2-microsoft_phi-4.diff\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test4-deprecated-material/result3-openai_gpt-oss-20b.diff\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test4-deprecated-material/result4-mistralai_devstral-small-2-2512.diff\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test4-deprecated-material/result5-google_gemma-3-27b.diff\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test4-deprecated-material/result6-qwen_qwen3-coder-30b.diff\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test4-deprecated-material/result7-nvidia_nemotron-3-nano.diff\n",
      "Saved to: /home/bazzite/IdeaProjects/notebooks/local-llm-for-android/notebooks/kotlin/../../build/test4-deprecated-material/execution-results.csv\n"
     ]
    },
    {
     "ename": "dev.langchain4j.exception.InvalidRequestException",
     "evalue": "{\"error\":\"Model unloaded.\"}",
     "output_type": "error",
     "traceback": [
      "dev.langchain4j.exception.InvalidRequestException: {\"error\":\"Model unloaded.\"}",
      "\tat dev.langchain4j.internal.ExceptionMapper$DefaultExceptionMapper.mapHttpStatusCode(ExceptionMapper.java:71)",
      "\tat dev.langchain4j.internal.ExceptionMapper$DefaultExceptionMapper.mapException(ExceptionMapper.java:44)",
      "\tat dev.langchain4j.internal.ExceptionMapper.withExceptionMapper(ExceptionMapper.java:31)",
      "\tat dev.langchain4j.internal.RetryUtils.lambda$withRetryMappingExceptions$1(RetryUtils.java:322)",
      "\tat dev.langchain4j.internal.RetryUtils$RetryPolicy.withRetry(RetryUtils.java:204)",
      "\tat dev.langchain4j.internal.RetryUtils.withRetry(RetryUtils.java:259)",
      "\tat dev.langchain4j.internal.RetryUtils.withRetryMappingExceptions(RetryUtils.java:322)",
      "\tat dev.langchain4j.internal.RetryUtils.withRetryMappingExceptions(RetryUtils.java:305)",
      "\tat dev.langchain4j.model.openai.OpenAiChatModel.doChat(OpenAiChatModel.java:149)",
      "\tat dev.langchain4j.model.chat.ChatModel.chat(ChatModel.java:46)",
      "\tat dev.langchain4j.guardrail.SynchronousChatExecutor.execute(SynchronousChatExecutor.java:32)",
      "\tat dev.langchain4j.guardrail.AbstractChatExecutor.execute(AbstractChatExecutor.java:45)",
      "\tat dev.langchain4j.service.DefaultAiServices$1.invoke(DefaultAiServices.java:372)",
      "\tat dev.langchain4j.service.DefaultAiServices$1.invoke(DefaultAiServices.java:227)",
      "\tat jdk.proxy8/jdk.proxy8.$Proxy56.generateCode(Unknown Source)",
      "\tat Line_38_jupyter.lambda$0$0$0(Line_38.jupyter.kts:43) at Cell In[36], line 43",
      "\tat Line_37_jupyter$ResourceMonitor.measurePeakDelta(Line_37.jupyter.kts:47) at Cell In[35], line 47",
      "\tat Line_38_jupyter.<init>(Line_38.jupyter.kts:43) at Cell In[36], line 43",
      "\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.evalWithConfigAndOtherScriptsResults(BasicJvmScriptEvaluator.kt:122)",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke$suspendImpl(BasicJvmScriptEvaluator.kt:48)",
      "\tat kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke(BasicJvmScriptEvaluator.kt)",
      "\tat kotlin.script.experimental.jvm.BasicJvmReplEvaluator.eval(BasicJvmReplEvaluator.kt:49)",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl$eval$resultWithDiagnostics$1.invokeSuspend(InternalEvaluatorImpl.kt:137)",
      "\tat kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:34)",
      "\tat kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:100)",
      "\tat kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:263)",
      "\tat kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:94)",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:70)",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)",
      "\tat kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:48)",
      "\tat kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.InternalEvaluatorImpl.eval(InternalEvaluatorImpl.kt:137)",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl.execute_W38Nk0s$lambda$0$1(CellExecutorImpl.kt:95)",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.ReplForJupyterImpl.withHost(ReplForJupyterImpl.kt:730)",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.CellExecutorImpl.execute-W38Nk0s(CellExecutorImpl.kt:93)",
      "\tat org.jetbrains.kotlinx.jupyter.repl.execution.CellExecutor.execute-W38Nk0s$default(CellExecutor.kt:14)",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.ReplForJupyterImpl.evaluateUserCode-wNURfNM(ReplForJupyterImpl.kt:591)",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.ReplForJupyterImpl.evalExImpl(ReplForJupyterImpl.kt:472)",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.ReplForJupyterImpl.evalEx$lambda$0(ReplForJupyterImpl.kt:466)",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.ReplForJupyterImpl.withEvalContext(ReplForJupyterImpl.kt:448)",
      "\tat org.jetbrains.kotlinx.jupyter.repl.impl.ReplForJupyterImpl.evalEx(ReplForJupyterImpl.kt:465)",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.IdeCompatibleMessageRequestProcessor.processExecuteRequest$lambda$0$0$0(IdeCompatibleMessageRequestProcessor.kt:161)",
      "\tat org.jetbrains.kotlinx.jupyter.streams.BlockingSubstitutionEngine.withDataSubstitution(SubstitutionEngine.kt:124)",
      "\tat org.jetbrains.kotlinx.jupyter.streams.StreamSubstitutionManager.withSubstitutedStreams(StreamSubstitutionManager.kt:118)",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.IdeCompatibleMessageRequestProcessor.withForkedIn(IdeCompatibleMessageRequestProcessor.kt:351)",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.IdeCompatibleMessageRequestProcessor.evalWithIO$lambda$0$0(IdeCompatibleMessageRequestProcessor.kt:364)",
      "\tat org.jetbrains.kotlinx.jupyter.streams.BlockingSubstitutionEngine.withDataSubstitution(SubstitutionEngine.kt:124)",
      "\tat org.jetbrains.kotlinx.jupyter.streams.StreamSubstitutionManager.withSubstitutedStreams(StreamSubstitutionManager.kt:118)",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.IdeCompatibleMessageRequestProcessor.withForkedErr(IdeCompatibleMessageRequestProcessor.kt:341)",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.IdeCompatibleMessageRequestProcessor.evalWithIO$lambda$0(IdeCompatibleMessageRequestProcessor.kt:363)",
      "\tat org.jetbrains.kotlinx.jupyter.streams.BlockingSubstitutionEngine.withDataSubstitution(SubstitutionEngine.kt:124)",
      "\tat org.jetbrains.kotlinx.jupyter.streams.StreamSubstitutionManager.withSubstitutedStreams(StreamSubstitutionManager.kt:118)",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.IdeCompatibleMessageRequestProcessor.withForkedOut(IdeCompatibleMessageRequestProcessor.kt:334)",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.IdeCompatibleMessageRequestProcessor.evalWithIO(IdeCompatibleMessageRequestProcessor.kt:362)",
      "\tat org.jetbrains.kotlinx.jupyter.messaging.IdeCompatibleMessageRequestProcessor.processExecuteRequest$lambda$0$0(IdeCompatibleMessageRequestProcessor.kt:160)",
      "\tat org.jetbrains.kotlinx.jupyter.execution.JupyterExecutorImpl$Task.execute(JupyterExecutorImpl.kt:41)",
      "\tat org.jetbrains.kotlinx.jupyter.execution.JupyterExecutorImpl.executorThread$lambda$0(JupyterExecutorImpl.kt:81)",
      "\tat kotlin.concurrent.ThreadsKt$thread$thread$1.run(Thread.kt:30)",
      "Caused by: dev.langchain4j.exception.HttpException: {\"error\":\"Model unloaded.\"}",
      "\tat dev.langchain4j.http.client.jdk.JdkHttpClient.execute(JdkHttpClient.java:53)",
      "\tat dev.langchain4j.model.openai.internal.SyncRequestExecutor.execute(SyncRequestExecutor.java:20)",
      "\tat dev.langchain4j.model.openai.internal.RequestExecutor.executeRaw(RequestExecutor.java:44)",
      "\tat dev.langchain4j.model.openai.OpenAiChatModel.lambda$doChat$0(OpenAiChatModel.java:150)",
      "\tat dev.langchain4j.internal.ExceptionMapper.withExceptionMapper(ExceptionMapper.java:29)",
      "\t... 58 more",
      "",
      "dev.langchain4j.exception.InvalidRequestException: {\"error\":\"Model unloaded.\"}",
      "at Cell In[36], line 43"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "2.2.20-Beta2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
